{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5. Perceptron for binary classification from scratch\n",
    "\n",
    "## This is a homework: put your names here\n",
    "\n",
    "- `First student name`\n",
    "- `Second student name`\n",
    "\n",
    "## Description\n",
    "\n",
    "The goal of this homework is to build and train a neural network *by hand,* much like we did in class.\n",
    "\n",
    "- To keep things as simple as possible we consider a simple *perceptron* with *one hidden layer*\n",
    "- We consider a binary-classification problem which cannot be solved with a linear classifier\n",
    "- We use a toy dataset which is already created for you with the `load_data()` function\n",
    "\n",
    "The neural network will have to be **programmed and trained from scratch**, meaning that you will have to define its structure yourself, and that **you are not allowed** to use the built-in functionalities of `PyTorch` for autodifferentiation. You will have to program the forward and backward pass *yourself*, together with the gradient descent algorithm.\n",
    "\n",
    "**Read the following instructions carefully in order to succeed**\n",
    "\n",
    "- You are already provided with a simple python class which will constitute your neural network. \n",
    "\n",
    "- The constructor of this class (`__init__`) requires you to specify the following parameters (investigate the data carefully in order to know how these parameters need to be defined).\n",
    "    - Number of input nodes (number of features)\n",
    "    - Number of hidden units \n",
    "    - Number of output nodes\n",
    "\n",
    "- The constructor of the neural network also requires you to allocate the weight matrices and bias vectors which will be the main components of your computational graph. For this, you can use some of the tensor operations we have seen in class, in order to initialize at random the weights, for instance.\n",
    "\n",
    "- Implementing a neural network consists in two main steps:\n",
    "    - Forward-pass: computing the output of the network based on its weight matrices and bias vectors. Since we consider binary classification, we should output activations that are activated by a sigmoid\n",
    "    - Backward-pass: computing how different the predictions of the network are from what should be predicted and update the parameters of the network with the backpropagation algorithm. You are already provided with a loss function in the code which requires only slight modifications.\n",
    "\n",
    "- If you want to challenge yourself you can extend the neural network as follows:\n",
    "    - add multiple hidden layers of different sizes\n",
    "    - explore different non-linear activation functions\n",
    "    - investigate whether the network starts overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T10:48:26.387197Z",
     "start_time": "2021-02-19T10:48:24.864492Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sklearn\n",
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T10:48:26.392694Z",
     "start_time": "2021-02-19T10:48:26.389535Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(n_samples=500):\n",
    "    return make_gaussian_quantiles(\n",
    "        mean=None,\n",
    "        cov=0.7,\n",
    "        n_samples=n_samples,\n",
    "        n_features=2,\n",
    "        n_classes=2,\n",
    "        shuffle=True,\n",
    "        random_state=42,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T10:48:26.402639Z",
     "start_time": "2021-02-19T10:48:26.395408Z"
    }
   },
   "outputs": [],
   "source": [
    "gaussian_quantiles = load_data()\n",
    "X, y = gaussian_quantiles\n",
    "\n",
    "print(X[:10, :])\n",
    "print(y[:10])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T10:48:27.301994Z",
     "start_time": "2021-02-19T10:48:27.131908Z"
    }
   },
   "outputs": [],
   "source": [
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "def plot_data(X, y, xy_labels=True, label=None, **kwargs):\n",
    "    X_1 = X[y == 1]\n",
    "    X_0 = X[y == 0]\n",
    "    if label is not None:\n",
    "        plt.scatter(X_1[:, 0], X_1[:, 1], c=\"blue\", s=30, label=label + \" (yi=1)\", **kwargs)\n",
    "        plt.scatter(X_0[:, 0], X_0[:, 1], c=\"red\", s=30, label=label + \" (yi=0)\", **kwargs)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.xlim(X[:, 0].min() - 1, X[:, 0].max() + 1)\n",
    "    plt.ylim(X[:, 1].min() - 1, X[:, 1].max() + 1)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plot_data(X_train, y_train, alpha=0.7, label=\"Train\")\n",
    "plot_data(X_test, y_test, alpha=0.3, label=\"Valid\")\n",
    "plt.legend(fontsize=13)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 5A.** What are the dimensions of the input? How many classes do we have? What do the classification labels correspond to? Look up the make_gaussian_quantiles function for the third question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A neural network class\n",
    "\n",
    "**QUESTION 5B.** Implement the `forward` and `backward` methods in the following class, and fill the `__init__` and `train` methods, in order to train the one-hidden layer perceptron for binary classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T09:25:41.859392Z",
     "start_time": "2021-02-19T09:25:41.854134Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, n_in, n_hidden, n_out):\n",
    "        # number of input nodes\n",
    "        self.n_x = n_in\n",
    "        # number of hidden nodes\n",
    "        self.n_h = n_hidden\n",
    "        # number of output nodes\n",
    "        self.n_y = n_out\n",
    "        # Define 1st weight matrix (using random initialization)\n",
    "        self.W1 = ?\n",
    "        # define 1st bias vector\n",
    "        self.b1 = ?\n",
    "        # Define 2nd weight matrix (using random initialization)\n",
    "        self.W2 = ?\n",
    "        # Define 2nd bias vector\n",
    "        self.b2 = \n",
    "                        \n",
    "    def forward(self, X):\n",
    "        pass\n",
    "        \n",
    "    def backward(self, X, y):\n",
    "        pass\n",
    "\n",
    "        \n",
    "    def train(self, X_train, y_train, X_valid, y_valid, epochs, learning_rate):\n",
    "        m = X_train.shape[0]\n",
    "        for e in range(epochs):\n",
    "            \n",
    "            self.forward()\n",
    "        \n",
    "            training_loss = -torch.sum(torch.mul(torch.log(?), y_train) + torch.mul(torch.log(1- ? ),  (1 - y_train))) / m # fill in the question marks \n",
    "        \n",
    "            self.backward()\n",
    "            \n",
    "            self.W1 -= # update of the 1st weight matrix\n",
    "            self.b1 -= # update of the 1st bias vector\n",
    "            self.W2 -= # update of the 2nd weight matrix\n",
    "            self.b2 -= # update of the 2nd bias vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 5C.** Train your neural network, modify your code to save the training and validation error along the gradient descent iterations and plot them. You should also print the train and validation errors during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T09:25:41.861250Z",
     "start_time": "2021-02-19T09:25:41.678Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_torch = torch.from_numpy(X_train).float()\n",
    "y_train_torch = torch.from_numpy(y_train).float()\n",
    "X_valid_torch = torch.from_numpy(X_test).float()\n",
    "y_valid_torch = torch.from_numpy(y_test).float()\n",
    "\n",
    "nn = NeuralNetwork(?, ?, ?)\n",
    "\n",
    "nn.train(\n",
    "    X_train_torch,\n",
    "    y_train_torch,\n",
    "    X_valid_torch,\n",
    "    y_valid_torch,\n",
    "    epochs=2500,\n",
    "    learning_rate=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 5D.** Modify the training code to display and save the true positives, false positives, true negatives, false negatives, precision, recall, and f1 score during training (that is, as epochs progress)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
